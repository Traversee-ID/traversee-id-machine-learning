{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://tmpfiles.org/1545912/tourism.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rGG_SCdKZcX1",
    "outputId": "961048d8-8f0f-4fb5-c1cb-1c11e4fe0343"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shortuuid in c:\\users\\hp\\anaconda3\\envs\\tf210\\lib\\site-packages (1.0.11)\n",
      "Requirement already satisfied: schedule in c:\\users\\hp\\anaconda3\\envs\\tf210\\lib\\site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install shortuuid\n",
    "!pip install schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "E-AnfN_gRs7O",
    "outputId": "5d006b54-362b-4a62-e30b-9ed5ffbd9c01"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>itemId</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>city</th>\n",
       "      <th>clicked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user1</td>\n",
       "      <td>7EzW8sTh9gaKg9UoBFacXX</td>\n",
       "      <td>Air terjun Gitgit adalah air terjun yang terle...</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user1</td>\n",
       "      <td>V5xEUjngBkMc5tTdiVKeGh</td>\n",
       "      <td>Air terjun Tegenungan adalah air terjun yang t...</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user1</td>\n",
       "      <td>G6nkGoXxWC95tkhcQn2i4Y</td>\n",
       "      <td>Alun-Alun Purworejo adalah sebuah alun-alun at...</td>\n",
       "      <td>5</td>\n",
       "      <td>140</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user1</td>\n",
       "      <td>GLXR7hBpCKsZUk3FgEqDvQ</td>\n",
       "      <td>Bali Safari &amp; Marine Park (BSMP) merupakan tem...</td>\n",
       "      <td>5</td>\n",
       "      <td>49</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user1</td>\n",
       "      <td>SEpL8LSMzbH6kXZPsRykkW</td>\n",
       "      <td>Batu Secret Zoo merupakan tempat wisata dan ke...</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>user5</td>\n",
       "      <td>daLxypzZZiYtPpH5Cdytwi</td>\n",
       "      <td>Sejak diresmikan pada bulan Desember 2017, Atl...</td>\n",
       "      <td>5</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>user5</td>\n",
       "      <td>EfZgg4m3Zn95W6icFjsaUy</td>\n",
       "      <td>Taman Hiburan Rakyat atau THR tentunya sudah t...</td>\n",
       "      <td>5</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4492</th>\n",
       "      <td>user5</td>\n",
       "      <td>NhhwKnhv9ZY6YYCx9NUUXv</td>\n",
       "      <td>Air mancur menari atau dancing fountain juga a...</td>\n",
       "      <td>5</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4493</th>\n",
       "      <td>user5</td>\n",
       "      <td>4oKKjyHRdfSxNyn9AWSk6Z</td>\n",
       "      <td>Taman Flora adalah salah satu taman kota di Su...</td>\n",
       "      <td>5</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4494</th>\n",
       "      <td>user5</td>\n",
       "      <td>Eh5XYu5FKcFYfJfB7Km8mH</td>\n",
       "      <td>Gereja Katolik Kelahiran Santa Perawan Maria m...</td>\n",
       "      <td>6</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4495 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     userId                  itemId  \\\n",
       "0     user1  7EzW8sTh9gaKg9UoBFacXX   \n",
       "1     user1  V5xEUjngBkMc5tTdiVKeGh   \n",
       "2     user1  G6nkGoXxWC95tkhcQn2i4Y   \n",
       "3     user1  GLXR7hBpCKsZUk3FgEqDvQ   \n",
       "4     user1  SEpL8LSMzbH6kXZPsRykkW   \n",
       "...     ...                     ...   \n",
       "4490  user5  daLxypzZZiYtPpH5Cdytwi   \n",
       "4491  user5  EfZgg4m3Zn95W6icFjsaUy   \n",
       "4492  user5  NhhwKnhv9ZY6YYCx9NUUXv   \n",
       "4493  user5  4oKKjyHRdfSxNyn9AWSk6Z   \n",
       "4494  user5  Eh5XYu5FKcFYfJfB7Km8mH   \n",
       "\n",
       "                                            description  category  city  \\\n",
       "0     Air terjun Gitgit adalah air terjun yang terle...         3    39   \n",
       "1     Air terjun Tegenungan adalah air terjun yang t...         3    46   \n",
       "2     Alun-Alun Purworejo adalah sebuah alun-alun at...         5   140   \n",
       "3     Bali Safari & Marine Park (BSMP) merupakan tem...         5    49   \n",
       "4     Batu Secret Zoo merupakan tempat wisata dan ke...         3    22   \n",
       "...                                                 ...       ...   ...   \n",
       "4490  Sejak diresmikan pada bulan Desember 2017, Atl...         5   159   \n",
       "4491  Taman Hiburan Rakyat atau THR tentunya sudah t...         5   159   \n",
       "4492  Air mancur menari atau dancing fountain juga a...         5   159   \n",
       "4493  Taman Flora adalah salah satu taman kota di Su...         5   159   \n",
       "4494  Gereja Katolik Kelahiran Santa Perawan Maria m...         6   159   \n",
       "\n",
       "      clicked  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "...       ...  \n",
       "4490      0.0  \n",
       "4491      0.0  \n",
       "4492      0.0  \n",
       "4493      0.0  \n",
       "4494      0.0  \n",
       "\n",
       "[4495 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import shortuuid\n",
    "\n",
    "def generate_dataset(file_name):\n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    # Now you can work with the DataFrame 'df'\n",
    "    df = df[['itemId', 'description', 'category', 'city']]\n",
    "\n",
    "    # Your JSON data\n",
    "    json_data = [\n",
    "        {\n",
    "            \"userId\": \"user1\",\n",
    "            \"clickedItems\": random.choices(df['itemId'], k=5)\n",
    "        },\n",
    "        {\n",
    "            \"userId\": \"user2\",\n",
    "            \"clickedItems\": random.choices(df['itemId'], k=5)\n",
    "        },\n",
    "        {\n",
    "            \"userId\": \"user3\",\n",
    "            \"clickedItems\": random.choices(df['itemId'], k=5)\n",
    "        },\n",
    "        {\n",
    "            \"userId\": \"user4\",\n",
    "            \"clickedItems\": random.choices(df['itemId'], k=5)\n",
    "        },\n",
    "        {\n",
    "            \"userId\": \"user5\",\n",
    "            \"clickedItems\": random.choices(df['itemId'], k=5)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Convert JSON to DataFrame\n",
    "    data = []\n",
    "    for user in json_data:\n",
    "        for item in user['clickedItems']:\n",
    "            data.append([user['userId'], item, 1])\n",
    "    df_user_clicked = pd.DataFrame(data, columns=['userId', 'itemId', 'clicked'])\n",
    "\n",
    "    # Create a DataFrame of all possible user-item pairs\n",
    "    all_users = df_user_clicked['userId'].unique()\n",
    "    all_items = df['itemId']\n",
    "\n",
    "    data_all = []\n",
    "    for user in all_users:\n",
    "        for item in all_items:\n",
    "            data_all.append([user, item])\n",
    "\n",
    "    df_all = pd.DataFrame(data_all, columns=['userId', 'itemId'])\n",
    "\n",
    "    # Merge df_all with df_items to add category, description, and other_attribute\n",
    "    df_all = pd.merge(df_all, df, on='itemId', how='left')\n",
    "\n",
    "    # Merge the user clicked data onto the DataFrame of all user-item pairs\n",
    "    df_final = pd.merge(df_all, df_user_clicked, how='left', on=['userId', 'itemId'], suffixes=('', '_user_clicked'))\n",
    "\n",
    "    # If the user has clicked the item, replace the 'clicked' value in df_all with the one from df_user_clicked\n",
    "    df_final['clicked'].fillna(0, inplace=True)  # fill NaNs with 0\n",
    "\n",
    "    return df_final\n",
    "\n",
    "df = generate_dataset('../data/tourism.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8s_RDtv6iS_6",
    "outputId": "490d852a-43d3-43e3-ba2a-696a524437f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.1185 - accuracy: 0.9942 - val_loss: 0.0480 - val_accuracy: 0.9922\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0299 - accuracy: 0.9950 - val_loss: 0.0489 - val_accuracy: 0.9922\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0226 - accuracy: 0.9950 - val_loss: 0.0508 - val_accuracy: 0.9922\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0142 - accuracy: 0.9950 - val_loss: 0.0571 - val_accuracy: 0.9922\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0118 - accuracy: 0.9950 - val_loss: 0.0640 - val_accuracy: 0.9922\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0114 - accuracy: 0.9950 - val_loss: 0.0674 - val_accuracy: 0.9922\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 1s 4ms/step - loss: 0.0113 - accuracy: 0.9950 - val_loss: 0.0707 - val_accuracy: 0.9922\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 1s 4ms/step - loss: 0.0112 - accuracy: 0.9950 - val_loss: 0.0727 - val_accuracy: 0.9922\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0111 - accuracy: 0.9950 - val_loss: 0.0751 - val_accuracy: 0.9922\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0109 - accuracy: 0.9950 - val_loss: 0.0747 - val_accuracy: 0.9922\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into a training set and a validation set\n",
    "df_train, df_val = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the maximum number of words in the texts to keep based on word frequency\n",
    "max_words = 500\n",
    "\n",
    "# Tokenizers\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df_train['description'])\n",
    "\n",
    "# Convert the texts to sequences\n",
    "description_sequences_train = tokenizer.texts_to_sequences(df_train['description'])\n",
    "description_sequences_val = tokenizer.texts_to_sequences(df_val['description'])\n",
    "\n",
    "# Pad the sequences so they are all the same length\n",
    "description_padded_train = pad_sequences(description_sequences_train, maxlen=max_words)\n",
    "description_padded_val = pad_sequences(description_sequences_val, maxlen=max_words)\n",
    "\n",
    "# Custom Label Encoding for user_id and item_id\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "encoded_user_ids_train = user_encoder.fit_transform(df_train['userId'])\n",
    "encoded_item_ids_train = item_encoder.fit_transform(df_train['itemId'])\n",
    "\n",
    "encoded_user_ids_val = user_encoder.transform(df_val['userId'])\n",
    "encoded_item_ids_val = item_encoder.transform(df_val['itemId'])\n",
    "\n",
    "labels_train = df_train['clicked']\n",
    "labels_val = df_val['clicked']\n",
    "\n",
    "# Build the model\n",
    "user_input = layers.Input(shape=(1,), name='user')\n",
    "item_input = layers.Input(shape=(1,), name='item')\n",
    "description_input = layers.Input(shape=(max_words,), name='description')\n",
    "\n",
    "user_embedding = layers.Embedding(input_dim=len(user_encoder.classes_), output_dim=50)(user_input)\n",
    "item_embedding = layers.Embedding(input_dim=len(item_encoder.classes_), output_dim=50)(item_input)\n",
    "description_embedding = layers.Embedding(input_dim=max_words, output_dim=50)(description_input)\n",
    "\n",
    "user_embedding = layers.Flatten()(user_embedding)\n",
    "item_embedding = layers.Flatten()(item_embedding)\n",
    "description_embedding = layers.GlobalAveragePooling1D()(description_embedding)\n",
    "\n",
    "concatenated = layers.Concatenate()([user_embedding, item_embedding, description_embedding])\n",
    "\n",
    "dense1 = layers.Dense(128, activation='relu')(concatenated)\n",
    "dense2 = layers.Dense(64, activation='relu')(dense1)\n",
    "out = layers.Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "model = tf.keras.Model(inputs=[user_input, item_input, description_input], outputs=out)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([encoded_user_ids_train, encoded_item_ids_train, description_padded_train], labels_train, epochs=10, validation_data=([encoded_user_ids_val, encoded_item_ids_val, description_padded_val], labels_val))\n",
    "\n",
    "# Save the model, label encoders, and tokenizers for future use\n",
    "model.save('recommendation_model.h5')\n",
    "np.save('user_encoder_classes.npy', user_encoder.classes_)\n",
    "np.save('item_encoder_classes.npy', item_encoder.classes_)\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q-pAQv9yqz-X",
    "outputId": "a921ab26-497d-44e0-d87f-7fdc11a9c21d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New user detected. Assigning random existing user for prediction.\n",
      "Top 10 recommendations for user4 are:\n",
      "Item: 7EBnBHULL5hzJHJtwhT3u8, predicted click probability: 0.39547663927078247\n",
      "Item: 7XJoM9gxMCkQgtoorP96Lu, predicted click probability: 0.39051663875579834\n",
      "Item: c2psmmoKXutCkAqQy2n7yH, predicted click probability: 0.3859640657901764\n",
      "Item: RJPfATFovFZ2TMtc3mgzRS, predicted click probability: 0.38472145795822144\n",
      "Item: CjDtzMwR5cpMQbPZvvMLSn, predicted click probability: 0.3813520073890686\n",
      "Item: gxpzMkYDfFhFVzTg2EmdxS, predicted click probability: 0.37987014651298523\n",
      "Item: 5ps3MEyuw8fPo7w4tXuGHD, predicted click probability: 0.3602381944656372\n",
      "Item: gLRuRSc7fZJh2fLpdwN3Yu, predicted click probability: 0.3513978123664856\n",
      "Item: aFYP6hxe6P42obyRVkcinR, predicted click probability: 0.3478638231754303\n",
      "Item: NEMGgEeMEkJFtyCdp5hdJX, predicted click probability: 0.346917986869812\n"
     ]
    }
   ],
   "source": [
    "import schedule\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class Recommender(tf.keras.Model):\n",
    "    def __init__(self, model_path, user_encoder_path, item_encoder_path, tokenizer_path, df, max_words):\n",
    "        super(Recommender, self).__init__()\n",
    "        self.model_path = model_path\n",
    "        self.user_encoder_path = user_encoder_path\n",
    "        self.item_encoder_path = item_encoder_path\n",
    "        self.tokenizer_path = tokenizer_path\n",
    "        self.df = df\n",
    "        self.max_words = max_words\n",
    "        self.reload_model()\n",
    "\n",
    "    def reload_model(self):\n",
    "        self.model = tf.keras.models.load_model(self.model_path)\n",
    "\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.user_encoder.classes_ = np.load(self.user_encoder_path, allow_pickle=True)\n",
    "\n",
    "        self.item_encoder = LabelEncoder()\n",
    "        self.item_encoder.classes_ = np.load(self.item_encoder_path, allow_pickle=True)\n",
    "\n",
    "        with open(self.tokenizer_path, 'rb') as handle:\n",
    "            self.tokenizer = pickle.load(handle)\n",
    "            \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=None, dtype=tf.string)])\n",
    "    def serving_default(self, new_user_id):\n",
    "        self.predict(new_user_id)\n",
    "\n",
    "    def predict(self, new_user_id):\n",
    "        if new_user_id not in self.user_encoder.classes_:\n",
    "            print(\"New user detected. Assigning random existing user for prediction.\")\n",
    "            new_user_id = np.random.choice(self.user_encoder.classes_)\n",
    "\n",
    "        all_item_ids = self.df['itemId'].unique().tolist()\n",
    "        all_descriptions = []\n",
    "\n",
    "        for item_id in all_item_ids:\n",
    "            item_data = self.df[self.df['itemId'] == item_id].iloc[0]\n",
    "            all_descriptions.append(item_data['description'])\n",
    "\n",
    "        encoded_new_user_id = self.user_encoder.transform([new_user_id]*len(all_item_ids))\n",
    "        encoded_all_item_ids = self.item_encoder.transform(all_item_ids)\n",
    "\n",
    "        description_sequences = self.tokenizer.texts_to_sequences(all_descriptions)\n",
    "        description_padded = pad_sequences(description_sequences, maxlen=self.max_words)\n",
    "\n",
    "        predictions = self.model.predict([encoded_new_user_id, encoded_all_item_ids, description_padded])\n",
    "        \n",
    "        top_10_indices = np.argsort(predictions[:, 0])[-10:]\n",
    "\n",
    "        print(\"Top 10 recommendations for\", new_user_id, \"are:\")\n",
    "        for index in reversed(top_10_indices):\n",
    "            print(f'Item: {all_item_ids[index]}, predicted click probability: {predictions[index][0]}')\n",
    "\n",
    "rec = Recommender('recommendation_model.h5', 'user_encoder_classes.npy', 'item_encoder_classes.npy', 'tokenizer.pickle', df, 500)\n",
    "\n",
    "def job():\n",
    "    rec.reload_model()\n",
    "\n",
    "# Schedule the task every day at 12am\n",
    "schedule.every().day.at(\"00:00\").do(job)\n",
    "\n",
    "#while True:\n",
    "#    schedule.run_pending()\n",
    "#    time.sleep(1)\n",
    "\n",
    "rec.predict(\"budiman\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
